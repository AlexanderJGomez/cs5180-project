{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "import numpy as np\n",
    "import gym  #requires OpenAI gym installed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "envs = {\n",
    "    'cartpole': gym.make('CartPole-v1'),\n",
    "    'bipedal-walker-v2': gym.make('BipedalWalker-v2')\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.training.saver.Saver object at 0x1367730b8>\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/pre_trained_step_99_0.000002_0.000010.ckpt\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "env = envs['bipedal-walker-v2']\n",
    "\n",
    "input_dims = 24\n",
    "state_placeholder = tf.placeholder(tf.float32, [None, input_dims]) \n",
    "\n",
    "def value_function(state):\n",
    "    n_hidden1 = 400  \n",
    "    n_hidden2 = 400\n",
    "    n_outputs = 1\n",
    "    \n",
    "    with tf.variable_scope(\"value_network\"):\n",
    "        init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        hidden1 = tf.layers.dense(state, n_hidden1, tf.nn.elu, init_xavier)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, tf.nn.elu, init_xavier) \n",
    "        V = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_network(state, n_outputs = 4):\n",
    "    n_hidden1 = 40\n",
    "    n_hidden2 = 40\n",
    "    \n",
    "    with tf.variable_scope(\"policy_network\"):\n",
    "        init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        hidden1 = tf.layers.dense(state, n_hidden1, tf.nn.elu, init_xavier)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, tf.nn.elu, init_xavier)\n",
    "        mu = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "        sigma = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "        sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "        norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "        action_tf_var = tf.squeeze(norm_dist.sample(1), axis=0)\n",
    "        action_tf_var = tf.clip_by_value(\n",
    "            action_tf_var, env.action_space.low[0], \n",
    "            env.action_space.high[0])\n",
    "    return action_tf_var, norm_dist\n",
    "\n",
    "# define required placeholders\n",
    "action_placeholder = tf.placeholder(tf.float32)\n",
    "delta_placeholder = tf.placeholder(tf.float32)\n",
    "target_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "action_tf_var, norm_dist = policy_network(state_placeholder)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    new_saver = tf.train.import_meta_graph('./tmp/pre_trained_step_1999_0.000020_0.000100.ckpt.meta')\n",
    "    print(new_saver)\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./tmp'))\n",
    "    env.render()\n",
    "    state = env.reset()   # state.shape -> (2,)\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while (not done):\n",
    "        #Sample action according to current policy\n",
    "        #action.shape = (1,4)\n",
    "        steps += 1\n",
    "        action  = sess.run(action_tf_var, feed_dict={\n",
    "                      state_placeholder: state.reshape(1, -1)})\n",
    "        #Execute action and observe reward & next state from E\n",
    "        next_state, reward, done, _ = env.step(\n",
    "                                np.squeeze(action, axis=0))\n",
    "        state = next_state\n",
    "        print(steps)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
